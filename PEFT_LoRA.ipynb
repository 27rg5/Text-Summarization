{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-27T05:05:58.932711Z","iopub.status.busy":"2024-06-27T05:05:58.931956Z","iopub.status.idle":"2024-06-27T05:06:12.205875Z","shell.execute_reply":"2024-06-27T05:06:12.203609Z","shell.execute_reply.started":"2024-06-27T05:05:58.932670Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.19.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.1)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (24.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.2\n"]}],"source":["!pip install peft\n","!pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes\n","!pip install trl\n","!pip install langchain\n","!pip install langchain_community\n","!pip install rouge-score\n","!pip install py7zr\n","!pip install evaluate"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-06-27T05:06:16.689352Z","iopub.status.busy":"2024-06-27T05:06:16.688558Z","iopub.status.idle":"2024-06-27T05:06:16.819713Z","shell.execute_reply":"2024-06-27T05:06:16.818636Z","shell.execute_reply.started":"2024-06-27T05:06:16.689318Z"},"trusted":true},"outputs":[],"source":["# Imports\n","import torch\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback, AutoModelForSequenceClassification\n","from datasets import load_dataset, load_metric, concatenate_datasets, DatasetDict\n","from sklearn.model_selection import train_test_split\n","from peft import get_peft_model, PeftModel, PeftConfig, LoraConfig, TaskType\n","from trl import PPOTrainer, PPOConfig\n","from langchain import PromptTemplate, LLMChain\n","from langchain.llms import HuggingFacePipeline\n","from tqdm import tqdm\n","from rouge_score import rouge_scorer\n","import numpy as np\n","from random import randrange\n","from evaluate import load\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-06-27T02:55:25.919035Z","iopub.status.busy":"2024-06-27T02:55:25.918674Z","iopub.status.idle":"2024-06-27T02:55:30.294071Z","shell.execute_reply":"2024-06-27T02:55:30.293058Z","shell.execute_reply.started":"2024-06-27T02:55:25.919004Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 14732\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 819\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 818\n","    })\n","})\n"]}],"source":["dataset = load_dataset(\"samsum\")\n","print(dataset)\n","\n","model_name = \"google/flan-t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-06-27T02:55:30.295914Z","iopub.status.busy":"2024-06-27T02:55:30.295646Z","iopub.status.idle":"2024-06-27T02:55:30.340271Z","shell.execute_reply":"2024-06-27T02:55:30.339302Z","shell.execute_reply.started":"2024-06-27T02:55:30.295889Z"},"trusted":true},"outputs":[],"source":["max_source_length = 512  \n","max_target_length = 128  \n","\n","# Preprocess function\n","def preprocess_function(sample, padding=\"max_length\"):\n","    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n","    \n","    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n","    \n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","    \n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","\n","tokenized_datasets = {}\n","for split in dataset.keys():\n","    tokenized_datasets[split] = dataset[split].map(\n","        preprocess_function,\n","        batched=True,\n","        remove_columns=[\"dialogue\", \"summary\", \"id\"]\n","    )"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-06-27T02:55:30.343954Z","iopub.status.busy":"2024-06-27T02:55:30.343687Z","iopub.status.idle":"2024-06-27T02:55:30.479161Z","shell.execute_reply":"2024-06-27T02:55:30.478248Z","shell.execute_reply.started":"2024-06-27T02:55:30.343930Z"},"trusted":true},"outputs":[],"source":["peft_config = LoraConfig(\n","    r=32, # Rank\n","    lora_alpha=32,\n","    target_modules=[\"q\", \"v\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_2_SEQ_LM \n",")\n","\n","peft_model = get_peft_model(base_model, peft_config)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-06-27T02:55:30.481393Z","iopub.status.busy":"2024-06-27T02:55:30.481036Z","iopub.status.idle":"2024-06-27T04:55:01.973727Z","shell.execute_reply":"2024-06-27T04:55:01.972735Z","shell.execute_reply.started":"2024-06-27T02:55:30.481358Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9210' max='9210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9210/9210 1:59:28, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.494200</td>\n","      <td>1.412887</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.422100</td>\n","      <td>1.400144</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.378900</td>\n","      <td>1.391535</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.338300</td>\n","      <td>1.389070</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.332500</td>\n","      <td>1.387629</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=9210, training_loss=1.3945142847448426, metrics={'train_runtime': 7169.4617, 'train_samples_per_second': 10.274, 'train_steps_per_second': 1.285, 'total_flos': 5.124003128672256e+16, 'train_loss': 1.3945142847448426, 'epoch': 5.0})"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./results\",\n","    auto_find_batch_size=True,\n","    num_train_epochs=5,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=1e-3,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"loss\",\n","    greater_is_better=False,\n","    logging_dir='./logs'\n",")\n","\n","# Initialize the trainer\n","trainer = Seq2SeqTrainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    tokenizer=tokenizer,\n",")\n","\n","# Start training\n","trainer.train()"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-06-27T04:55:18.284707Z","iopub.status.busy":"2024-06-27T04:55:18.284352Z","iopub.status.idle":"2024-06-27T04:55:18.555116Z","shell.execute_reply":"2024-06-27T04:55:18.554078Z","shell.execute_reply.started":"2024-06-27T04:55:18.284682Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["trainer.model.save_pretrained('./lora_model')"]},{"cell_type":"markdown","metadata":{},"source":["### Qualitative evaluation"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-06-27T05:12:53.880113Z","iopub.status.busy":"2024-06-27T05:12:53.879766Z","iopub.status.idle":"2024-06-27T05:12:54.910235Z","shell.execute_reply":"2024-06-27T05:12:54.909229Z","shell.execute_reply.started":"2024-06-27T05:12:53.880069Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["input sentence: Tomas: Has anybody received the grant yet?\n","Sierra: no, not yet\n","Jeremy: I haven't checked even\n","Tomas: I'm completely broke\n","Tomas: checking my bank account every hour\n","Tomas: but nothing's happening there\n","Sierra: lol\n","Sierra: be patient. If you need money I can lend you some, don't worry\n","Tomas: Thanks, I hope they'll arrive any minute\n","------------------------------------------------------------\n","Summary by base model:\n","Sierra hasn't received the grant yet. Jeremy hasn't checked yet and \n","\n","Summary by peft model:\n","Tomas has not received the grant yet. Sierra will lend him some money if needed\n"]}],"source":["peft_model.eval()\n","sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n","\n","input_ids = tokenizer(\"summarize: \" + sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n","print(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n","outputs = base_model.generate(input_ids=input_ids, do_sample=True, top_p=0.9)\n","print(f\"Summary by base model:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]} \\n\")\n","\n","outputs = peft_model.generate(input_ids=input_ids, do_sample=True, top_p=0.9)\n","print(f\"Summary by peft model:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Quantitative evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ROUGE metric to access quantitatively\n","rouge = load('rouge')\n","\n","# Function to compute ROUGE scores\n","def compute_rouge_scores(predictions, references):\n","    return rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n","\n","# Function to generate summaries\n","def generate_summaries(model, tokenizer, dataset):\n","    summaries = []\n","    for example in tqdm(dataset):\n","        input_text = \"summarize: \" + example['dialogue']\n","        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n","        \n","        with torch.no_grad():\n","            outputs = model.generate(**inputs, do_sample=True)\n","        \n","        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        summaries.append(summary)\n","    return summaries\n","\n","# Generate summaries using base model\n","print(\"Generating summaries with base model...\")\n","base_summaries = generate_summaries(base_model, tokenizer, dataset['test'])\n","\n","# Compute ROUGE scores for base model\n","base_rouge_scores = compute_rouge_scores(base_summaries, dataset['test']['summary'])\n","print(\"Base Model ROUGE Scores:\")\n","print(base_rouge_scores)\n","\n","# Generate summaries using PEFT model\n","print(\"Generating summaries with PEFT model...\")\n","peft_summaries = generate_summaries(peft_model, tokenizer, dataset['test'])\n","\n","# Compute ROUGE scores for PEFT model\n","peft_rouge_scores = compute_rouge_scores(peft_summaries, dataset['test']['summary'])\n","print(\"PEFT Model ROUGE Scores:\")\n","print(peft_rouge_scores)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
